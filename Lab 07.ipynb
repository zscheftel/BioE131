{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create 100 MB sets of random data containng varying amounts of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p100 = np.random.choice([0, 1], size=8*1024*1024*100, replace=True, p=[1, 0])\n",
    "p100 = np.packbits(p100)\n",
    "\n",
    "p90 = np.random.choice([0, 1], size=8*1024*1024*100, replace=True, p=[0.9, 0.1])\n",
    "p90 = np.packbits(p90)\n",
    "\n",
    "p80 = np.random.choice([0, 1], size=8*1024*1024*100, replace=True, p=[0.8, 0.2])\n",
    "p80 = np.packbits(p80)\n",
    "\n",
    "p70 = np.random.choice([0, 1], size=8*1024*1024*100, replace=True, p=[0.7, 0.3])\n",
    "p70 = np.packbits(p70)\n",
    "\n",
    "p60 = np.random.choice([0, 1], size=8*1024*1024*100, replace=True, p=[0.6, 0.4])\n",
    "p60 = np.packbits(p60)\n",
    "\n",
    "p50 = np.random.choice([0, 1], size=8*1024*1024*100, replace=True, p=[0.5, 0.5])\n",
    "p50 = np.packbits(p50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"p100\", \"wb\").write(p100)\n",
    "open(\"p90\", \"wb\").write(p90)\n",
    "open(\"p80\", \"wb\").write(p80)\n",
    "open(\"p70\", \"wb\").write(p70)\n",
    "open(\"p60\", \"wb\").write(p60)\n",
    "open(\"p50\", \"wb\").write(p50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create DNA and protein sequences at 100 million letters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    temp = np.random.choice([\"A\", \"T\", \"G\", \"C\"], size=100000000, replace=True, p=[0.25, 0.25, 0.25, 0.25])\n",
    "    open(\"seq\" + str(i) + \".fa\", \"w\").write(\"\".join(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commands similar to the following were run on terminal:\n",
    "\n",
    "```\n",
    "time gzip –k zeros_100p\n",
    "time bzip2 –k zeros_100p\n",
    "time pbzip2 –k zeros_100p\n",
    "time ArithmeticCompresszeros_100p zeros_100p.art\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p100</th>\n",
       "      <th>p90</th>\n",
       "      <th>p80</th>\n",
       "      <th>p70</th>\n",
       "      <th>p60</th>\n",
       "      <th>p50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gzip</th>\n",
       "      <td>105MB, 102 kB, 0.717s</td>\n",
       "      <td>105MB, 58.7MB, 18.49s</td>\n",
       "      <td>105MB, 81.2MB, 13.407s</td>\n",
       "      <td>105MB, 93.6MB, 6.026s</td>\n",
       "      <td>105MB, 102MB, 4.373s</td>\n",
       "      <td>105MB, 102MB, 3.655s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bzip2</th>\n",
       "      <td>105MB, 113B, 1.020s</td>\n",
       "      <td>105MB, 61.7MB, 10.670s</td>\n",
       "      <td>105MB, 86.6MB, 11.941s</td>\n",
       "      <td>105MB, 99.8MB, 13.769s</td>\n",
       "      <td>105MB, 105MB, 15.721s</td>\n",
       "      <td>105MB, 105MB, 16.730s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pbzip2</th>\n",
       "      <td>105MB, 5.62KB,0.104s</td>\n",
       "      <td>105MB, 61.2MB, 0.777s</td>\n",
       "      <td>105MB, 86.7MB, 0.981s</td>\n",
       "      <td>105MB, 99.8MB, 1.175s</td>\n",
       "      <td>105MB, 105MB, 1.433s</td>\n",
       "      <td>105MB, 105MB, 1.451s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ArthComp</th>\n",
       "      <td>105MB, 1.03KB, 14.82s</td>\n",
       "      <td>105MB, 49.2MB, 28.822s</td>\n",
       "      <td>105MB, 75.7MB, 35.296s</td>\n",
       "      <td>105MB, 92.4MB, 39.202s</td>\n",
       "      <td>105MB, 102MB, 41.034s</td>\n",
       "      <td>105MB, 105MB, 40.891s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           p100                     p90  \\\n",
       "gzip      105MB, 102 kB, 0.717s   105MB, 58.7MB, 18.49s   \n",
       "bzip2       105MB, 113B, 1.020s  105MB, 61.7MB, 10.670s   \n",
       "pbzip2     105MB, 5.62KB,0.104s   105MB, 61.2MB, 0.777s   \n",
       "ArthComp  105MB, 1.03KB, 14.82s  105MB, 49.2MB, 28.822s   \n",
       "\n",
       "                             p80                     p70  \\\n",
       "gzip      105MB, 81.2MB, 13.407s   105MB, 93.6MB, 6.026s   \n",
       "bzip2     105MB, 86.6MB, 11.941s  105MB, 99.8MB, 13.769s   \n",
       "pbzip2     105MB, 86.7MB, 0.981s   105MB, 99.8MB, 1.175s   \n",
       "ArthComp  105MB, 75.7MB, 35.296s  105MB, 92.4MB, 39.202s   \n",
       "\n",
       "                            p60                    p50  \n",
       "gzip       105MB, 102MB, 4.373s   105MB, 102MB, 3.655s  \n",
       "bzip2     105MB, 105MB, 15.721s  105MB, 105MB, 16.730s  \n",
       "pbzip2     105MB, 105MB, 1.433s   105MB, 105MB, 1.451s  \n",
       "ArthComp  105MB, 102MB, 41.034s  105MB, 105MB, 40.891s  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'p100': [\"105MB, 102 kB, 0.717s\", \"105MB, 113B, 1.020s\", \"105MB, 5.62KB, 0.104s\",\"105MB, 1.03KB, 14.82s\"],\n",
    "     'p90' : [\"105MB, 58.7MB, 18.49s\", \"105MB, 61.7MB, 10.670s\", \"105MB, 61.2MB, 0.777s\", \"105MB, 49.2MB, 28.822s\"],\n",
    "     'p80' : [\"105MB, 81.2MB, 13.407s\", \"105MB, 86.6MB, 11.941s\", \"105MB, 86.7MB, 0.981s\", \"105MB, 75.7MB, 35.296s\"],\n",
    "     'p70' : [\"105MB, 93.6MB, 6.026s\", \"105MB, 99.8MB, 13.769s\", \"105MB, 99.8MB, 1.175s\", \"105MB, 92.4MB, 39.202s\"],\n",
    "     'p60' : [\"105MB, 102MB, 4.373s\", \"105MB, 105MB, 15.721s\", \"105MB, 105MB, 1.433s\", \"105MB, 102MB, 41.034s\"],\n",
    "     'p50' : [\"105MB, 102MB, 3.655s\", \"105MB, 105MB, 16.730s\", \"105MB, 105MB, 1.451s\", \"105MB, 105MB, 40.891s\"],\n",
    "    }\n",
    "dF = pd.DataFrame(data=d, index=['gzip', 'bzip2', 'pbzip2', 'ArthComp'])\n",
    "dF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq0</th>\n",
       "      <th>seq1</th>\n",
       "      <th>seq2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gzip</th>\n",
       "      <td>100MB, 29.2MB, 12.140s</td>\n",
       "      <td>100MB, 29.2MB, 12.154s</td>\n",
       "      <td>100MB, 29.2MB, 12.142s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bzip2</th>\n",
       "      <td>100MB, 27.3MB, 9.487s</td>\n",
       "      <td>100MB, 27.3MB, 10.132s</td>\n",
       "      <td>100MB, 27.3MB, 9.650s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pbzip2</th>\n",
       "      <td>100MB, 27.3MB, 0.682s</td>\n",
       "      <td>100MB, 27.3MB, 0.671s</td>\n",
       "      <td>100MB, 27.3MB, 0.674s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ArthComp</th>\n",
       "      <td>100MB, 25MB, 21.435s</td>\n",
       "      <td>100MB, 25MB, 21.573s</td>\n",
       "      <td>100MB, 25MB, 21.552s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            seq0                    seq1  \\\n",
       "gzip      100MB, 29.2MB, 12.140s  100MB, 29.2MB, 12.154s   \n",
       "bzip2      100MB, 27.3MB, 9.487s  100MB, 27.3MB, 10.132s   \n",
       "pbzip2     100MB, 27.3MB, 0.682s   100MB, 27.3MB, 0.671s   \n",
       "ArthComp    100MB, 25MB, 21.435s    100MB, 25MB, 21.573s   \n",
       "\n",
       "                            seq2  \n",
       "gzip      100MB, 29.2MB, 12.142s  \n",
       "bzip2      100MB, 27.3MB, 9.650s  \n",
       "pbzip2     100MB, 27.3MB, 0.674s  \n",
       "ArthComp    100MB, 25MB, 21.552s  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = {'seq0': [\"100MB, 29.2MB, 12.140s\", \"100MB, 27.3MB, 9.487s\", \"100MB, 27.3MB, 0.682s\",\"100MB, 25MB, 21.435s\"],\n",
    "     'seq1' : [\"100MB, 29.2MB, 12.154s\", \"100MB, 27.3MB, 10.132s\", \"100MB, 27.3MB, 0.671s\", \"100MB, 25MB, 21.573s\"],\n",
    "     'seq2' : [\"100MB, 29.2MB, 12.142s\", \"100MB, 27.3MB, 9.650s\", \"100MB, 27.3MB, 0.674s\", \"100MB, 25MB, 21.552s\"],\n",
    "    }\n",
    "dF2 = pd.DataFrame(data=d1, index=['gzip', 'bzip2', 'pbzip2', 'ArthComp'])\n",
    "dF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm achieves the best level of compression on each file type?\n",
    "In order of 100% to 50% of zeroes, the best algorithms are bzip2, ArthComp, ArthComp, ArthComp, gzip, gzip.\n",
    "For the DNA sequences, the best compression algorithm is tied between bzip2 and pbzip2.\n",
    "\n",
    "Which algorithm is the fastest?\n",
    "pzip2 runs the fastest.\n",
    "\n",
    "What is the difference between bzip2 and pbzip2? Do you expect one to be faster and why?\n",
    "pbzip2 runs significantly faster than bzip2. bzip2 compresses files of mostly zeroes better than pbzip2. It makes sense that pbzip is faster because it runs in parallel.\n",
    "\n",
    "How does the level of compression change as the percentage of zeros increases? Why does this happen?\n",
    "The compression of the file becomes less and less effective as the percentage of zeroes increases. This is because there are fewer sections of uninterrupted entries of the same kind that could be compressed.\n",
    "\n",
    "What is the minimum number of bits required to store a single DNA base?\n",
    "0.25 bits because 25MB are sufficient to encode 100MB of DNA data.\n",
    "\n",
    "What is the minimum number of bits required to store an amino acid letter?\n",
    "4.322 according to lecture slides.\n",
    "\n",
    "In your tests, how many bits did gzip and bzip2 actually require to store your random DNA and protein sequences?\n",
    "gzip and bzip2 took 29.2 and 27.3 MB respectively.\n",
    "\n",
    "Are gzip and bzip2 performing well on DNA and proteins?\n",
    "They are doing really quite well, they are just a few MB off from the ideal 25MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find 20 sequences of gp120 homologs from HIV isolates and concatenate them into one fasta file gp120.fasta.  We expect it to achieve better compression than random data since homologous sequences will have large sections of similar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "Entrez.email = '15liforrest@berkeley.edu'\n",
    "handle = Entrez.esearch(db='nucleotide',\n",
    "                       term = 'gp120 HIV',\n",
    "                       sort = 'relevance',\n",
    "                       idtype = 'acc')\n",
    "with open('gp120.fasta', 'w') as fasta_handle:\n",
    "    for i in Entrez.read(handle)['IdList']:\n",
    "        handle = Entrez.efetch(db='nucleotide', id=i, rettype = 'fasta', retmode='text')\n",
    "        new_handle = SeqIO.parse(handle, 'fasta')\n",
    "        SeqIO.write(new_handle, fasta_handle, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gp120</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gzip</th>\n",
       "      <td>12.1kB, 1.45kB, 0.003s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bzip2</th>\n",
       "      <td>12.1kB, 1.58kB, 0.008s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ArthComp</th>\n",
       "      <td>12.1kB, 5.7kB, 0.014s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           gp120\n",
       "gzip      12.1kB, 1.45kB, 0.003s\n",
       "bzip2     12.1kB, 1.58kB, 0.008s\n",
       "ArthComp   12.1kB, 5.7kB, 0.014s"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3 = {'gp120': [\"12.1kB, 1.45kB, 0.003s\", \"12.1kB, 1.58kB, 0.008s\", \"12.1kB, 5.7kB, 0.014s\"],\n",
    "}\n",
    "dF3 = pd.DataFrame(data=d3, index=['gzip', 'bzip2', 'ArthComp'])\n",
    "dF3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that we get compression ratios ranging from around 0.11-0.13 for gzip and bzip2, but a ratio of 0.46 for ArithmeticCompress.  For gzip and bzip2, this ratio is about half that than for random data, which had compression ratios of around 0.25-0.30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For re-sequencing of similar genomes, we should be using pbzip2 for parallel compression.\n",
    "\n",
    "For protein sequences, we find that using pbzip2 would also work fastest among the algorithms, and provide approximately the same compression as gzip2.\n",
    "\n",
    "For completely random binary microscope images, we have found that for the most random sequences, gzip2 provides marginally the best compression.\n",
    "\n",
    "Thus, we can see approximately 85% data storage savings on 80% of the data, 70% data storage savings when compressing the 10% protein sequences, and at worst 3% savings when compressing the 10% random images.  Thus, we estimate a savings of around 75.3% data storage and thus a $37500 bonus for every day this compression scheme is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
